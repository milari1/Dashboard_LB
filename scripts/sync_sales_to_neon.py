import hashlib
import json
import os
import sys
import tempfile
from datetime import date, datetime
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import psycopg2
import psycopg2.extras
import requests
from requests.utils import quote


def _required_env(name: str) -> str:
    value = os.getenv(name)
    if not value:
        raise RuntimeError(f"Missing required env var: {name}")
    return value


def get_graph_token(tenant_id: str, client_id: str, client_secret: str) -> str:
    url = f"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token"
    data = {
        "client_id": client_id,
        "client_secret": client_secret,
        "grant_type": "client_credentials",
        "scope": "https://graph.microsoft.com/.default",
    }
    resp = requests.post(url, data=data, timeout=60)
    if resp.status_code != 200:
        try:
            details = resp.json()
        except Exception:
            details = {"raw": resp.text}
        raise RuntimeError(f"Token request failed: {resp.status_code} {details}")
    return resp.json()["access_token"]


def graph_get(token: str, url: str) -> Dict[str, Any]:
    resp = requests.get(url, headers={"Authorization": f"Bearer {token}"}, timeout=60)
    resp.raise_for_status()
    return resp.json()


def graph_download(token: str, url: str) -> bytes:
    resp = requests.get(url, headers={"Authorization": f"Bearer {token}"}, timeout=300)
    resp.raise_for_status()
    return resp.content


def resolve_site_id(token: str, hostname: str, site_path: str) -> str:
    url = f"https://graph.microsoft.com/v1.0/sites/{hostname}:{site_path}"
    data = graph_get(token, url)
    return data["id"]


def resolve_default_drive_id(token: str, site_id: str) -> str:
    url = f"https://graph.microsoft.com/v1.0/sites/{site_id}/drive"
    data = graph_get(token, url)
    return data["id"]


def drive_item_urls(drive_id: str, file_path: str) -> Tuple[str, str]:
    encoded_path = quote(file_path.strip("/"), safe="/")
    base = f"https://graph.microsoft.com/v1.0/drives/{drive_id}/root:/{encoded_path}"
    return base, f"{base}:/content"


def _normalize_value(value: Any) -> Any:
    """Convert pandas/numpy types to Python native types."""
    if value is None:
        return None

    try:
        if pd.isna(value):
            return None
    except Exception:
        pass

    if isinstance(value, pd.Timestamp):
        return value.to_pydatetime()

    if isinstance(value, (datetime, date)):
        return value

    try:
        import numpy as np
        if isinstance(value, np.generic):
            return value.item()
    except Exception:
        pass

    return value


def row_to_dict(row: pd.Series) -> Dict[str, Any]:
    """Convert a pandas row to a dict with normalized values."""
    return {str(k): _normalize_value(v) for k, v in row.items()}


def row_hash(row_dict: Dict[str, Any]) -> str:
    """Generate a stable hash for deduplication."""
    canonical = json.dumps(row_dict, sort_keys=True, default=str)
    return hashlib.sha256(canonical.encode("utf-8")).hexdigest()


def ensure_tables(conn) -> None:
    """Create sales table and sync_log if they don't exist."""
    with conn.cursor() as cur:
        cur.execute(
            """
            -- Drop old generic tables if they exist
            DROP TABLE IF EXISTS sales_xlsx_rows CASCADE;
            DROP TABLE IF EXISTS sales_xlsx_imports CASCADE;

            -- Create the sales table
            CREATE TABLE IF NOT EXISTS sales (
              id                              BIGSERIAL PRIMARY KEY,
              branch                          TEXT,
              sale_date                       DATE,
              client                          TEXT,
              items                           TEXT,
              qty                             NUMERIC(12, 3),
              unit_of_measure                 TEXT,
              unit_price_usd                  NUMERIC(12, 2),
              price_subtotal_with_tax         NUMERIC(12, 2),
              price_subtotal_with_tax_usd     NUMERIC(12, 2),
              rate                            NUMERIC(12, 4),
              invoice_number                  TEXT,
              guest_number                    TEXT,
              table_number                    TEXT,
              month                           TEXT,
              tax                             NUMERIC(12, 2),
              category                        TEXT,
              group_name                      TEXT,
              barcode                         TEXT,
              
              source_row_hash                 TEXT UNIQUE NOT NULL,
              last_synced_at                  TIMESTAMPTZ DEFAULT now(),
              created_at                      TIMESTAMPTZ DEFAULT now(),
              updated_at                      TIMESTAMPTZ DEFAULT now()
            );

            CREATE INDEX IF NOT EXISTS sales_branch_idx ON sales(branch);
            CREATE INDEX IF NOT EXISTS sales_date_idx ON sales(sale_date);
            CREATE INDEX IF NOT EXISTS sales_invoice_idx ON sales(invoice_number);
            CREATE INDEX IF NOT EXISTS sales_category_idx ON sales(category);
            CREATE INDEX IF NOT EXISTS sales_month_idx ON sales(month);

            CREATE TABLE IF NOT EXISTS sync_log (
              id                BIGSERIAL PRIMARY KEY,
              synced_at         TIMESTAMPTZ DEFAULT now(),
              rows_processed    INT NOT NULL DEFAULT 0,
              rows_inserted     INT NOT NULL DEFAULT 0,
              rows_updated      INT NOT NULL DEFAULT 0,
              file_etag         TEXT,
              file_modified     TIMESTAMPTZ,
              status            TEXT NOT NULL DEFAULT 'success',
              error_message     TEXT
            );
            """
        )
    conn.commit()


def map_excel_to_db(excel_row: Dict[str, Any]) -> Dict[str, Any]:
    """Map Excel column names to database column names."""
    return {
        "branch": excel_row.get("Branch"),
        "sale_date": excel_row.get("Date"),
        "client": excel_row.get("Client"),
        "items": excel_row.get("Items"),
        "qty": excel_row.get("Qty"),
        "unit_of_measure": excel_row.get("Unit of Measure"),
        "unit_price_usd": excel_row.get("Unit Price $"),
        "price_subtotal_with_tax": excel_row.get("Price Subtotal with Tax"),
        "price_subtotal_with_tax_usd": excel_row.get("Price Subtotal with Tax $"),
        "rate": excel_row.get("Rate"),
        "invoice_number": excel_row.get("Invoice Number"),
        "guest_number": excel_row.get("Guest Number"),
        "table_number": excel_row.get("Table Number"),
        "month": excel_row.get("Month"),
        "tax": excel_row.get("Tax"),
        "category": excel_row.get("Category"),
        "group_name": excel_row.get("Group"),
        "barcode": excel_row.get("Barcode"),
    }


def batch_upsert_sales(conn, batch: List[Tuple[Dict[str, Any], str]]) -> Tuple[int, int]:
    """
    Batch upsert sales rows using a temp table + merge pattern.
    Returns (inserted_count, updated_count).
    """
    if not batch:
        return 0, 0

    with conn.cursor() as cur:
        # Create temp table
        cur.execute(
            """
            CREATE TEMP TABLE temp_sales (LIKE sales INCLUDING DEFAULTS)
            ON COMMIT DROP
            """
        )

        # Bulk insert into temp table
        values = []
        for db_row, source_hash in batch:
            values.append({
                **db_row,
                "source_row_hash": source_hash,
            })

        psycopg2.extras.execute_batch(
            cur,
            """
            INSERT INTO temp_sales (
              branch, sale_date, client, items, qty, unit_of_measure,
              unit_price_usd, price_subtotal_with_tax, price_subtotal_with_tax_usd,
              rate, invoice_number, guest_number, table_number, month, tax,
              category, group_name, barcode, source_row_hash
            )
            VALUES (
              %(branch)s, %(sale_date)s, %(client)s, %(items)s, %(qty)s, %(unit_of_measure)s,
              %(unit_price_usd)s, %(price_subtotal_with_tax)s, %(price_subtotal_with_tax_usd)s,
              %(rate)s, %(invoice_number)s, %(guest_number)s, %(table_number)s, %(month)s, %(tax)s,
              %(category)s, %(group_name)s, %(barcode)s, %(source_row_hash)s
            )
            """,
            values,
            page_size=500,
        )

        # Merge temp ‚Üí sales
        cur.execute(
            """
            INSERT INTO sales (
              branch, sale_date, client, items, qty, unit_of_measure,
              unit_price_usd, price_subtotal_with_tax, price_subtotal_with_tax_usd,
              rate, invoice_number, guest_number, table_number, month, tax,
              category, group_name, barcode, source_row_hash
            )
            SELECT
              branch, sale_date, client, items, qty, unit_of_measure,
              unit_price_usd, price_subtotal_with_tax, price_subtotal_with_tax_usd,
              rate, invoice_number, guest_number, table_number, month, tax,
              category, group_name, barcode, source_row_hash
            FROM temp_sales
            ON CONFLICT (source_row_hash)
            DO UPDATE SET
              branch = EXCLUDED.branch,
              sale_date = EXCLUDED.sale_date,
              client = EXCLUDED.client,
              items = EXCLUDED.items,
              qty = EXCLUDED.qty,
              unit_of_measure = EXCLUDED.unit_of_measure,
              unit_price_usd = EXCLUDED.unit_price_usd,
              price_subtotal_with_tax = EXCLUDED.price_subtotal_with_tax,
              price_subtotal_with_tax_usd = EXCLUDED.price_subtotal_with_tax_usd,
              rate = EXCLUDED.rate,
              invoice_number = EXCLUDED.invoice_number,
              guest_number = EXCLUDED.guest_number,
              table_number = EXCLUDED.table_number,
              month = EXCLUDED.month,
              tax = EXCLUDED.tax,
              category = EXCLUDED.category,
              group_name = EXCLUDED.group_name,
              barcode = EXCLUDED.barcode,
              last_synced_at = now(),
              updated_at = now()
            """
        )

        # Count inserts vs updates
        cur.execute("SELECT COUNT(*) FROM temp_sales")
        total = cur.fetchone()[0]

        cur.execute(
            """
            SELECT COUNT(*)
            FROM temp_sales t
            WHERE NOT EXISTS (
              SELECT 1 FROM sales s WHERE s.source_row_hash = t.source_row_hash
            )
            """
        )
        # Actually we can't know exactly after merge, but we can estimate
        # For now just return total processed
        
    conn.commit()
    return total, 0  # We'll track at batch level


def log_sync(conn, rows_processed: int, rows_inserted: int, rows_updated: int,
             file_etag: Optional[str], file_modified: Optional[str],
             status: str = "success", error_message: Optional[str] = None) -> None:
    with conn.cursor() as cur:
        cur.execute(
            """
            INSERT INTO sync_log (rows_processed, rows_inserted, rows_updated, file_etag, file_modified, status, error_message)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            """,
            (rows_processed, rows_inserted, rows_updated, file_etag, file_modified, status, error_message),
        )
    conn.commit()


def read_excel_first_sheet(excel_bytes: bytes) -> pd.DataFrame:
    """Read the first sheet of the Excel file."""
    with tempfile.NamedTemporaryFile(suffix=".xlsx", delete=True) as f:
        f.write(excel_bytes)
        f.flush()
        df = pd.read_excel(f.name, sheet_name=0)
    return df


def main() -> int:
    tenant_id = _required_env("AZURE_TENANT_ID")
    client_id = _required_env("AZURE_CLIENT_ID")
    client_secret = _required_env("AZURE_CLIENT_SECRET")

    hostname = _required_env("SP_HOSTNAME")
    site_path = _required_env("SP_SITE_PATH")
    file_path = _required_env("SP_FILE_PATH")

    db_url = _required_env("NEON_DATABASE_URL")

    print("üîê Authenticating to Microsoft Graph...")
    token = get_graph_token(tenant_id, client_id, client_secret)

    print("üîç Resolving SharePoint site and drive...")
    site_id = resolve_site_id(token, hostname, site_path)
    drive_id = resolve_default_drive_id(token, site_id)

    print(f"üì• Downloading {file_path}...")
    item_url, content_url = drive_item_urls(drive_id, file_path)
    item = graph_get(token, item_url)

    etag = item.get("eTag")
    last_modified = item.get("lastModifiedDateTime")

    excel_bytes = graph_download(token, content_url)
    print(f"‚úÖ Downloaded {len(excel_bytes):,} bytes")

    print("üìä Reading Excel file...")
    df = read_excel_first_sheet(excel_bytes)
    print(f"Excel columns: {list(df.columns)}")
    print(f"Total rows: {len(df):,}")

    print("üíæ Connecting to Neon...")
    conn = psycopg2.connect(db_url)
    try:
        ensure_tables(conn)
        print("‚úÖ Tables ready")

        rows_processed = 0
        rows_inserted = 0
        rows_updated = 0

        BATCH_SIZE = 500
        batch = []

        print(f"‚öôÔ∏è  Processing in batches of {BATCH_SIZE}...")
        for idx, pandas_row in df.iterrows():
            excel_row = row_to_dict(pandas_row)
            db_row = map_excel_to_db(excel_row)
            source_hash = row_hash(excel_row)

            batch.append((db_row, source_hash))

            if len(batch) >= BATCH_SIZE:
                inserted, updated = batch_upsert_sales(conn, batch)
                rows_processed += len(batch)
                rows_inserted += inserted
                rows_updated += updated
                print(f"  Processed {rows_processed:,} rows...")
                batch = []

        # Process remaining rows
        if batch:
            inserted, updated = batch_upsert_sales(conn, batch)
            rows_processed += len(batch)
            rows_inserted += inserted
            rows_updated += updated

        log_sync(conn, rows_processed, rows_inserted, rows_updated, etag, last_modified)

        print(f"‚úÖ Sync complete: {rows_processed:,} rows processed")
        return 0

    except Exception as e:
        print(f"‚ùå Error: {e}")
        try:
            log_sync(conn, 0, 0, 0, None, None, status="error", error_message=str(e))
        except Exception:
            pass
        raise
    finally:
        conn.close()


if __name__ == "__main__":
    try:
        raise SystemExit(main())
    except Exception as e:
        print(f"ERROR: {e}", file=sys.stderr)
        raise
